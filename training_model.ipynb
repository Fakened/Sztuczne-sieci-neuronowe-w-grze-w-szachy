{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chess\n",
    "import chess.engine\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, Flatten, Input\n",
    "from collections import deque\n",
    "import random\n",
    "import csv\n",
    "import h5py\n",
    "import time\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consts and hiperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 10000\n",
    "GAMMA = 0.95\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-4  \n",
    "BATCH_SIZE = 64\n",
    "MEMORY_SIZE = 10000\n",
    "SAVE_INTERVAL = 50  \n",
    "TAU_MAX = 1.0   \n",
    "TAU_MIN = 0.1   \n",
    "TAU_DECAY = 0.9995  \n",
    "MAX_DEPTH = 16 \n",
    "NUM_THREADS = 4  \n",
    "TIME_LIMIT = 2.0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural nertwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape=(8, 8, 12)):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(Conv2D(64, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(Conv2D(64, kernel_size=3, activation='relu', padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    optimizer = tf.keras.optimizers.AdamW(learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class Node for MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, board, parent=None, move=None, agent=None):\n",
    "        self.board = board.copy()\n",
    "        self.parent = parent\n",
    "        self.move = move\n",
    "        self.children = []\n",
    "        self.visits = 0\n",
    "        self.value = 0.0\n",
    "        self.agent = agent\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.children) == len(list(self.board.legal_moves))\n",
    "\n",
    "    def best_child(self):\n",
    "        with self.lock:\n",
    "            visits = np.array([child.visits for child in self.children], dtype=np.float32)\n",
    "            q_values = np.array([child.value / (child.visits + 1e-8) for child in self.children])\n",
    "            probabilities = self.agent.softmax(q_values)\n",
    "            if probabilities is None or len(probabilities) == 0:\n",
    "                return random.choice(self.children)\n",
    "            return np.random.choice(self.children, p=probabilities)\n",
    "\n",
    "    def expand(self):\n",
    "        with self.lock:\n",
    "            tried_moves = [child.move for child in self.children]\n",
    "            legal_moves = list(self.board.legal_moves)\n",
    "            for move in legal_moves:\n",
    "                if move not in tried_moves:\n",
    "                    new_board = self.board.copy()\n",
    "                    new_board.push(move)\n",
    "                    child_node = Node(new_board, parent=self, move=move, agent=self.agent)\n",
    "                    self.children.append(child_node)\n",
    "                    return child_node\n",
    "            return None  # Brak niepróbowanych ruchów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chess agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessAgent:\n",
    "    def __init__(self):\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "        self.tau = TAU_MAX\n",
    "        self.model = build_model()\n",
    "        self.transposition_table = {}\n",
    "        self.tree_lock = threading.Lock()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        if len(x) == 0:\n",
    "            return None \n",
    "        z = x - np.max(x)\n",
    "        e_x = np.exp(z / self.tau)\n",
    "        softmax_values = e_x / e_x.sum()\n",
    "        if np.any(np.isnan(softmax_values)) or np.any(np.isinf(softmax_values)):\n",
    "            return np.ones_like(e_x) / len(e_x)\n",
    "        return softmax_values\n",
    "\n",
    "    def act_with_mcts_and_softmax(self, board, time_limit=TIME_LIMIT, num_threads=NUM_THREADS, max_depth=MAX_DEPTH):\n",
    "        root = Node(board, agent=self)\n",
    "        root.visits = 1 \n",
    "        start_time = time.time()\n",
    "\n",
    "        def run_simulation():\n",
    "            node = root\n",
    "            while node.is_fully_expanded() and not node.board.is_game_over():\n",
    "                node = node.best_child()\n",
    "            if not node.board.is_game_over():\n",
    "                node = node.expand()\n",
    "                if node is None:\n",
    "                    return  \n",
    "            result = self.simulate(node.board, max_depth=max_depth)\n",
    "            self.backpropagate(node, result)\n",
    "\n",
    "        threads = []\n",
    "        while time.time() - start_time < time_limit:\n",
    "            if len(threads) < num_threads:\n",
    "                thread = threading.Thread(target=run_simulation)\n",
    "                thread.start()\n",
    "                threads.append(thread)\n",
    "            threads = [t for t in threads if t.is_alive()]\n",
    "            time.sleep(0.01)\n",
    "\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "\n",
    "        visits = np.array([child.visits for child in root.children], dtype=np.float32)\n",
    "        probabilities = self.softmax(visits)\n",
    "        if probabilities is None or len(root.children) == 0:\n",
    "            return random.choice(list(board.legal_moves))\n",
    "        best_child = np.random.choice(root.children, p=probabilities)\n",
    "        return best_child.move\n",
    "\n",
    "    def backpropagate(self, node, result):\n",
    "        while node is not None:\n",
    "            with node.lock:\n",
    "                node.visits += 1\n",
    "                node.value += result\n",
    "            node = node.parent\n",
    "\n",
    "    def simulate(self, board, max_depth=MAX_DEPTH):\n",
    "        current_board = board.copy()\n",
    "        depth = 0\n",
    "        while not current_board.is_game_over() and depth < max_depth:\n",
    "            legal_moves = list(current_board.legal_moves)\n",
    "            if not legal_moves:\n",
    "                break\n",
    "            move = random.choice(legal_moves)\n",
    "            current_board.push(move)\n",
    "            depth += 1\n",
    "        result = self.evaluate_state(current_board)\n",
    "        return result\n",
    "\n",
    "    def evaluate_state(self, board):\n",
    "        board_fen = board.fen()\n",
    "        if board_fen in self.transposition_table:\n",
    "            return self.transposition_table[board_fen]\n",
    "        else:\n",
    "            input_state = self.state_to_input(board)\n",
    "            value = self.model.predict(input_state, verbose=0)[0][0]\n",
    "            self.transposition_table[board_fen] = value\n",
    "            return value\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, BATCH_SIZE)\n",
    "        states = []\n",
    "        targets = []\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                next_state_value = self.model.predict(next_state, verbose=0)[0][0]\n",
    "                target = reward + GAMMA * next_state_value\n",
    "            target_f = self.model.predict(state, verbose=0)\n",
    "            target_f[0][0] = target\n",
    "            states.append(state[0])\n",
    "            targets.append(target_f[0])\n",
    "        states = np.array(states)\n",
    "        targets = np.array(targets)\n",
    "        self.model.fit(states, targets, epochs=1, verbose=0)\n",
    "        if self.tau > TAU_MIN:\n",
    "            self.tau *= TAU_DECAY\n",
    "\n",
    "    def state_to_input(self, board):\n",
    "        planes = np.zeros((8, 8, 12))\n",
    "        for square in chess.SQUARES:\n",
    "            piece = board.piece_at(square)\n",
    "            if piece:\n",
    "                piece_type = piece.piece_type - 1\n",
    "                color = 0 if piece.color == chess.WHITE else 6\n",
    "                row = 7 - chess.square_rank(square)\n",
    "                col = chess.square_file(square)\n",
    "                planes[row, col, piece_type + color] = 1\n",
    "        return np.expand_dims(planes, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward for agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def material_count(board):\n",
    "    piece_values = {\n",
    "        chess.PAWN: 1,\n",
    "        chess.KNIGHT: 3,\n",
    "        chess.BISHOP: 3,\n",
    "        chess.ROOK: 5,\n",
    "        chess.QUEEN: 9,\n",
    "        chess.KING: 0\n",
    "    }\n",
    "    material = 0\n",
    "    for square in chess.SQUARES:\n",
    "        piece = board.piece_at(square)\n",
    "        if piece:\n",
    "            value = piece_values[piece.piece_type]\n",
    "            material += value if piece.color == chess.WHITE else -value\n",
    "    return material\n",
    "\n",
    "def get_reward(board, previous_material_count):\n",
    "    reward = 0\n",
    "    if board.is_checkmate():\n",
    "        reward = 1 if board.turn == chess.BLACK else -1\n",
    "    elif board.is_stalemate() or board.is_insufficient_material():\n",
    "        reward = 0\n",
    "    else:\n",
    "        current_material_count = material_count(board)\n",
    "        material_difference = current_material_count - previous_material_count\n",
    "        reward += 0.1 * material_difference\n",
    "        center_squares = [chess.D4, chess.E4, chess.D5, chess.E5]\n",
    "        for square in center_squares:\n",
    "            piece = board.piece_at(square)\n",
    "            if piece and piece.color == board.turn:\n",
    "                reward += 0.05\n",
    "        reward += 0.01 * len(list(board.legal_moves))\n",
    "        if board.is_check():\n",
    "            reward -= 0.3\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_state(agent, episode, model_save_path, stockfish_level):\n",
    "    model_save_filename = f\"{model_save_path}{episode}.h5\"\n",
    "    agent.model.save(model_save_filename)\n",
    "    with h5py.File(model_save_filename, 'a') as h5file:\n",
    "        h5file.attrs['tau'] = agent.tau\n",
    "        h5file.attrs['episode'] = episode\n",
    "        h5file.attrs['stockfish_level'] = stockfish_level\n",
    "    print(f\"Model i stan treningu zapisany po epizodzie {episode}\")\n",
    "\n",
    "def load_training_state(agent, model_save_path, after_episode=0):\n",
    "    model_filename = f\"{model_save_path}{after_episode}.h5\"\n",
    "    if os.path.exists(model_filename):\n",
    "        print(f\"Wczytywanie modelu z pliku: {model_filename}\")\n",
    "        agent.model = load_model(model_filename, compile=False)\n",
    "        optimizer = tf.keras.optimizers.AdamW(learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "        agent.model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "        with h5py.File(model_filename, 'r') as h5file:\n",
    "            agent.tau = h5file.attrs.get('tau', TAU_MAX)\n",
    "            episode = h5file.attrs['episode']\n",
    "            stockfish_level = h5file.attrs['stockfish_level']\n",
    "        print(f\"Wczytano stan treningu: epizod {episode}, TAU: {agent.tau}, poziom Stockfisha: {stockfish_level}\")\n",
    "    else:\n",
    "        print(\"Brak zapisanego stanu treningu.\")\n",
    "        episode = 0\n",
    "        stockfish_level = 1\n",
    "    return episode, stockfish_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent, episodes=EPISODES, stockfish_path=\"stockfish/stockfish-windows-x86-64.exe\",\n",
    "               save_interval=SAVE_INTERVAL, model_save_path=\"saved_model/\", load_existing_model=False, stats_path=\"stats.csv\", after_episode=0):\n",
    "    if not os.path.exists(model_save_path):\n",
    "        os.makedirs(model_save_path)\n",
    "    if not os.path.exists(stats_path):\n",
    "        with open(stats_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Episode', 'Result', 'Moves', 'Avg Reward', 'TAU'])\n",
    "    if load_existing_model:\n",
    "        start_episode, stockfish_level = load_training_state(agent, model_save_path, after_episode)\n",
    "    else:\n",
    "        start_episode = 0\n",
    "        stockfish_level = 1\n",
    "    engine = chess.engine.SimpleEngine.popen_uci(stockfish_path)\n",
    "    engine.configure({\"Skill Level\": stockfish_level})\n",
    "    for e in range(start_episode, episodes):\n",
    "        board = chess.Board()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        moves_in_game = 0\n",
    "        previous_material_count = material_count(board)\n",
    "        while not done:\n",
    "            if board.turn == chess.WHITE:\n",
    "                state = agent.state_to_input(board)\n",
    "                action = agent.act_with_mcts_and_softmax(board, time_limit=TIME_LIMIT, num_threads=NUM_THREADS, max_depth=MAX_DEPTH)\n",
    "                board.push(action)\n",
    "                done = board.is_game_over()\n",
    "                next_state = agent.state_to_input(board)\n",
    "                reward = get_reward(board, previous_material_count)\n",
    "                previous_material_count = material_count(board)\n",
    "                total_reward += reward\n",
    "                agent.remember(state, action, reward, next_state, done)\n",
    "                moves_in_game += 1\n",
    "            else:\n",
    "                result = engine.play(board, chess.engine.Limit(time=0.1))\n",
    "                board.push(result.move)\n",
    "                done = board.is_game_over()\n",
    "        agent.replay()\n",
    "        print(f\"Epizod {e+1}/{episodes}, Wynik: {result_str}, Ruchy: {moves_in_game}, \"\n",
    "              f\"Średnia nagroda: {avg_reward:.2f}, TAU: {agent.tau:.4f}, \"\n",
    "              f\"Poziom Stockfisha: {stockfish_level}\")\n",
    "    engine.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ChessAgent()\n",
    "train_agent(agent, episodes=EPISODES, model_save_path='saved_model/', load_existing_model=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
